
import com.google.cloud.bigquery.{Field, StandardSQLTypeName, Schema}
import org.apache.spark.sql.types._
import scala.jdk.CollectionConverters._

import com.google.cloud.bigquery.{Field, StandardSQLTypeName, Schema}
import org.apache.spark.sql.types._
import scala.jdk.CollectionConverters._

def convertToBigQueryField(field: StructField): Field = {
  val mode = if (field.nullable) Field.Mode.NULLABLE else Field.Mode.REQUIRED

  field.dataType match {
    // Primitive Types
    case IntegerType | LongType =>
      Field.newBuilder(field.name, StandardSQLTypeName.INT64).setMode(mode).build()

    case DoubleType | FloatType =>
      Field.newBuilder(field.name, StandardSQLTypeName.FLOAT64).setMode(mode).build()

    case StringType =>
      Field.newBuilder(field.name, StandardSQLTypeName.STRING).setMode(mode).build()

    case BooleanType =>
      Field.newBuilder(field.name, StandardSQLTypeName.BOOL).setMode(mode).build()

    case TimestampType =>
      Field.newBuilder(field.name, StandardSQLTypeName.TIMESTAMP).setMode(mode).build()

    case DateType =>
      Field.newBuilder(field.name, StandardSQLTypeName.DATE).setMode(mode).build()

    case d: DecimalType =>
      if (d.precision <= 38 && d.scale <= 9)
        Field.newBuilder(field.name, StandardSQLTypeName.NUMERIC).setMode(mode).build()
      else
        Field.newBuilder(field.name, StandardSQLTypeName.STRING).setMode(mode).build()

    // ArrayType: build array with element type
    case ArrayType(elementType, _) =>
      val elementField = convertToBigQueryField(StructField(field.name + "_element", elementType))
      Field.newBuilder(field.name, elementField.getType).setMode(Field.Mode.REPEATED).build()

    // StructType: build nested struct
    case StructType(fields) =>
      val subFields = fields.map(convertToBigQueryField).toList.asJava
      Field.newBuilder(field.name, Field.Type.record(subFields)).setMode(mode).build()

    // MapType: must be converted in DataFrame before reaching here
    case MapType(keyType, valueType, _) =>
      throw new IllegalArgumentException(s"MapType is not supported directly: $field.name. Please convert it to Array<Struct<key, value>> before schema creation.")

    // Fallback
    case _ =>
      Field.newBuilder(field.name, StandardSQLTypeName.STRING).setMode(mode).build()
  }
}







---------------------------------------------
def convertToBigQueryField(field: StructField): Field = {
  val fieldType = field.dataType match {
    case IntegerType | LongType =>
      StandardSQLTypeName.INT64
    case DoubleType | FloatType =>
      StandardSQLTypeName.FLOAT64
    case StringType =>
      StandardSQLTypeName.STRING
    case BooleanType =>
      StandardSQLTypeName.BOOL
    case TimestampType =>
      StandardSQLTypeName.TIMESTAMP
    case DateType =>
      StandardSQLTypeName.DATE
    case d: DecimalType =>
      if (d.precision <= 38 && d.scale <= 9) StandardSQLTypeName.NUMERIC
      else StandardSQLTypeName.STRING
    case ArrayType(elementType, _) =>
      val elementField = convertToBigQueryField(StructField(field.name, elementType))
      Field.newBuilder(field.name, StandardSQLTypeName.ARRAY, elementField.getSubFields)
          .setMode(if (field.nullable) Field.Mode.NULLABLE else Field.Mode.REQUIRED)
          .build()
    case StructType(fields) =>
      val subFields = fields.map(convertToBigQueryField).toList.asJava
      Field.newBuilder(field.name, StandardSQLTypeName.STRUCT, subFields)
          .setMode(if (field.nullable) Field.Mode.NULLABLE else Field.Mode.REQUIRED)
          .build()
    case _ =>
      StandardSQLTypeName.STRING
  }

  if (!field.dataType.isInstanceOf[ArrayType] && !field.dataType.isInstanceOf[StructType]) {
    Field.newBuilder(field.name, fieldType)
      .setMode(if (field.nullable) Field.Mode.NULLABLE else Field.Mode.REQUIRED)
      .build()
  } else {
    // Already built above
    null
  }
}

==========================================


import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

def mapToStructArrayGeneric(df: org.apache.spark.sql.DataFrame): org.apache.spark.sql.DataFrame = {
  val transformedCols = df.schema.fields.map { field =>
    field.dataType match {
      case MapType(_, _, _) =>
        map_entries(col(field.name)).alias(field.name)
      case _ =>
        col(field.name)
    }
  }
  df.select(transformedCols: _*)
}
