Here’s how you can create sample data to generate an RDD of ConsumerRecord[String, GenericRecord] in Scala:

Step-by-Step Code to Create Sample RDD

import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.avro.generic.{GenericData, GenericRecord}
import org.apache.avro.Schema
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD

// Define a function to create the RDD
def createSampleRDD(sc: SparkContext): RDD[ConsumerRecord[String, GenericRecord]] = {
  // Define the Avro schema
  val schemaString =
    """
      |{
      | "type": "record",
      | "name": "TestRecord",
      | "fields": [
      |   {"name": "field1", "type": "string"},
      |   {"name": "field2", "type": "int"}
      | ]
      |}
    """.stripMargin
  val schema = new Schema.Parser().parse(schemaString)

  // Create sample GenericRecord data
  val record1 = new GenericData.Record(schema)
  record1.put("field1", "value1")
  record1.put("field2", 100)

  val record2 = new GenericData.Record(schema)
  record2.put("field1", "value2")
  record2.put("field2", 200)

  // Create ConsumerRecord instances with the GenericRecord as value
  val consumerRecord1 = new ConsumerRecord[String, GenericRecord](
    "topic1", // topic
    0,        // partition
    1L,       // offset
    "key1",   // key
    record1   // value
  )

  val consumerRecord2 = new ConsumerRecord[String, GenericRecord](
    "topic1", // topic
    0,        // partition
    2L,       // offset
    "key2",   // key
    record2   // value
  )

  // Create an RDD from the ConsumerRecords
  sc.parallelize(Seq(consumerRecord1, consumerRecord2))
}

// Usage example (assuming SparkContext exists as `sc`):
// val messagesRDD = createSampleRDD(sc)

Explanation of the Code
	1.	Define Avro Schema:
	•	Used a JSON string to define a simple Avro schema with fields field1 (string) and field2 (int).
	•	Parsed it into an Avro Schema object.
	2.	Create GenericRecord Data:
	•	Created GenericRecord objects (record1 and record2) with the defined schema.
	•	Populated the records with sample data.
	3.	Create ConsumerRecord Instances:
	•	Created Kafka ConsumerRecord objects with:
	•	String as the key.
	•	GenericRecord as the value.
	•	Sample values for topic, partition, offset, and key.
	4.	Create RDD:
	•	Used sc.parallelize to create an RDD containing the sample ConsumerRecord objects.

Example Output

The resulting RDD contains ConsumerRecord objects like:

ConsumerRecord(topic = topic1, partition = 0, offset = 1, key = key1, value = {"field1": "value1", "field2": 100})
ConsumerRecord(topic = topic1, partition = 0, offset = 2, key = key2, value = {"field1": "value2", "field2": 200})

How to Use

You can use this RDD as the input messagesRDD for testing your getDataFrameFromMessageRdd method. For example:

val messagesRDD = createSampleRDD(sc)
val resultDF = getDataFrameFromMessageRdd(spark, messagesRDD)

// Validate the DataFrame schema and data as needed
resultDF.show()

Let me know if you need further clarification or modifications!

----------
The error java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord occurs because the ConsumerRecord class is not serializable. Spark requires all data in an RDD to be serializable so that it can distribute the data across the cluster.

To resolve this issue, you need to transform the ConsumerRecord objects into a serializable format (e.g., a case class or a simple Row object) before creating the RDD.

Solution: Transform ConsumerRecord into Serializable Data

Here’s how you can resolve the issue:

Step 1: Define a Case Class for Serializable Data

Define a case class to represent the serialized version of ConsumerRecord:

case class SerializableConsumerRecord(
    key: String,
    value: String,
    topic: String,
    partition: Int,
    offset: Long,
    timestamp: Long,
    timestampType: Int
)

Step 2: Transform ConsumerRecord into SerializableConsumerRecord

Transform the ConsumerRecord[String, GenericRecord] into SerializableConsumerRecord before creating the RDD:

import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.avro.generic.{GenericData, GenericRecord}
import org.apache.avro.Schema
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD

// Define a function to create the RDD
def createSampleRDD(sc: SparkContext): RDD[SerializableConsumerRecord] = {
  // Define the Avro schema
  val schemaString =
    """
      |{
      | "type": "record",
      | "name": "TestRecord",
      | "fields": [
      |   {"name": "field1", "type": "string"},
      |   {"name": "field2", "type": "int"}
      | ]
      |}
    """.stripMargin
  val schema = new Schema.Parser().parse(schemaString)

  // Create sample GenericRecord data
  val record1 = new GenericData.Record(schema)
  record1.put("field1", "value1")
  record1.put("field2", 100)

  val record2 = new GenericData.Record(schema)
  record2.put("field1", "value2")
  record2.put("field2", 200)

  // Create ConsumerRecord instances
  val consumerRecord1 = new ConsumerRecord[String, GenericRecord](
    "topic1", 0, 1L, "key1", record1
  )

  val consumerRecord2 = new ConsumerRecord[String, GenericRecord](
    "topic1", 0, 2L, "key2", record2
  )

  // Transform ConsumerRecord to SerializableConsumerRecord
  val serializableRecords = Seq(
    SerializableConsumerRecord(
      consumerRecord1.key(),
      consumerRecord1.value().toString,
      consumerRecord1.topic(),
      consumerRecord1.partition(),
      consumerRecord1.offset(),
      System.currentTimeMillis(),
      consumerRecord1.timestampType().id
    ),
    SerializableConsumerRecord(
      consumerRecord2.key(),
      consumerRecord2.value().toString,
      consumerRecord2.topic(),
      consumerRecord2.partition(),
      consumerRecord2.offset(),
      System.currentTimeMillis(),
      consumerRecord2.timestampType().id
    )
  )

  // Create an RDD from the SerializableConsumerRecord objects
  sc.parallelize(serializableRecords)
}

Step 3: Use the Serializable RDD in Your Test

Now, use the createSampleRDD function to create an RDD of SerializableConsumerRecord and pass it to your test.

Updated Test Case Example

test("getDataFrameFromMessageRdd processes SerializableConsumerRecord RDD") {
  // Define the schema
  val kafkaGenericMsgSchema = StructType(
    List(
      StructField("key", StringType, false),
      StructField("value", StringType, false),
      StructField("Kafka_topic", StringType, false),
      StructField("Kafka_partitionId", IntegerType, false),
      StructField("Kafka_offset", LongType, false),
      StructField("Kafka_CreateTime", LongType, false),
      StructField("timestampType", IntegerType, false)
    )
  )

  // Create SparkSession
  val spark = SparkSession.builder()
    .appName("Test")
    .master("local[*]")
    .getOrCreate()

  // Create the RDD of SerializableConsumerRecord
  val sc = spark.sparkContext
  val messagesRDD = createSampleRDD(sc)

  // Transform the SerializableConsumerRecord RDD into Rows
  val rowsRDD = messagesRDD.map(r => Row(
    r.key, r.value, r.topic, r.partition, r.offset, r.timestamp, r.timestampType
  ))

  // Create the DataFrame
  val resultDF = spark.createDataFrame(rowsRDD, kafkaGenericMsgSchema)

  // Assertions
  assert(resultDF.schema == kafkaGenericMsgSchema)
  resultDF.show()

  spark.stop()
}

Explanation of Changes
	1.	Serializable Case Class:
	•	The ConsumerRecord is transformed into a serializable SerializableConsumerRecord case class.
	2.	RDD Transformation:
	•	The RDD of ConsumerRecord is converted into SerializableConsumerRecord before parallelizing.
	3.	Schema Alignment:
	•	The SerializableConsumerRecord fields match the StructType schema (kafkaGenericMsgSchema).

This approach avoids the NotSerializableException and ensures that the data in the RDD is compatible with Spark. Let me know if you need further clarification!

======================================================

import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.avro.generic.{GenericData, GenericRecord}
import org.apache.avro.Schema
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD

// Function to create RDD[ConsumerRecord[String, GenericRecord]]
def createConsumerRecordRDD(sc: SparkContext): RDD[ConsumerRecord[String, GenericRecord]] = {
  // Define the Avro schema
  val schemaString =
    """
      |{
      | "type": "record",
      | "name": "TestRecord",
      | "fields": [
      |   {"name": "field1", "type": "string"},
      |   {"name": "field2", "type": "int"}
      | ]
      |}
    """.stripMargin
  val schema = new Schema.Parser().parse(schemaString)

  // Create sample GenericRecord data
  val record1 = new GenericData.Record(schema)
  record1.put("field1", "value1")
  record1.put("field2", 100)

  val record2 = new GenericData.Record(schema)
  record2.put("field1", "value2")
  record2.put("field2", 200)

  // Create ConsumerRecord instances
  val consumerRecord1 = new ConsumerRecord[String, GenericRecord](
    "topic1", // topic
    0,        // partition
    1L,       // offset
    "key1",   // key
    record1   // value
  )

  val consumerRecord2 = new ConsumerRecord[String, GenericRecord](
    "topic1", // topic
    0,        // partition
    2L,       // offset
    "key2",   // key
    record2   // value
  )

  // Create an RDD directly from the ConsumerRecord objects
  sc.parallelize(Seq(consumerRecord1, consumerRecord2))
}

// Usage Example
// Assuming `sc` is your SparkContext
// val messagesRDD = createConsumerRecordRDD(sc)

test("getDataFrameFromMessageRdd processes RDD[ConsumerRecord[String, GenericRecord]]") {
  val spark = SparkSession.builder()
    .appName("Test")
    .master("local[*]")
    .getOrCreate()

  val sc = spark.sparkContext

  // Create the RDD of ConsumerRecord[String, GenericRecord]
  val messagesRDD = createConsumerRecordRDD(sc)

  // Call the method under test
  val resultDF = getDataFrameFromMessageRdd(spark, messagesRDD)

  // Assertions
  resultDF.show() // Print the resulting DataFrame for debugging

  // Verify schema and data
  assert(resultDF.schema.fields.map(_.name).toSeq == Seq(
    "key", "value", "Kafka_topic", "Kafka_partitionId", "Kafka_offset", "Kafka_CreateTime", "timestampType"
  ))

  spark.stop()
}
//aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa

import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.avro.generic.{GenericData, GenericRecord}
import org.apache.avro.Schema
import org.apache.spark.SparkContext
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types._
import org.apache.spark.rdd.RDD

// Define a case class for serialized ConsumerRecord
case class SerializableConsumerRecord(
    key: String,
    value: String,
    topic: String,
    partition: Int,
    offset: Long,
    timestamp: Long,
    timestampType: Int
)

test("getDataFrameFromMessageRdd processes RDD[ConsumerRecord[String, GenericRecord]] correctly") {
  // Create SparkSession
  val spark = SparkSession.builder()
    .appName("Test")
    .master("local[*]")
    .getOrCreate()

  val sc = spark.sparkContext

  // Define the Avro schema
  val schemaString =
    """
      |{
      | "type": "record",
      | "name": "TestRecord",
      | "fields": [
      |   {"name": "field1", "type": "string"},
      |   {"name": "field2", "type": "int"}
      | ]
      |}
    """.stripMargin
  val avroSchema = new Schema.Parser().parse(schemaString)

  // Create sample GenericRecord data
  val record1 = new GenericData.Record(avroSchema)
  record1.put("field1", "value1")
  record1.put("field2", 100)

  val record2 = new GenericData.Record(avroSchema)
  record2.put("field1", "value2")
  record2.put("field2", 200)

  // Create ConsumerRecord instances
  val consumerRecord1 = new ConsumerRecord[String, GenericRecord](
    "topic1", // topic
    0,        // partition
    1L,       // offset
    "key1",   // key
    record1   // value
  )

  val consumerRecord2 = new ConsumerRecord[String, GenericRecord](
    "topic1", // topic
    0,        // partition
    2L,       // offset
    "key2",   // key
    record2   // value
  )

  // Create RDD[ConsumerRecord[String, GenericRecord]]
  val messagesRDD: RDD[ConsumerRecord[String, GenericRecord]] = sc.parallelize(Seq(consumerRecord1, consumerRecord2))

  // Call the method under test: getDataFrameFromMessageRdd
  val resultDF = getDataFrameFromMessageRdd(spark, messagesRDD)

  // Define the expected schema
  val expectedSchema = StructType(
    List(
      StructField("key", StringType, false),
      StructField("value", StringType, false),
      StructField("Kafka_topic", StringType, false),
      StructField("Kafka_partitionId", IntegerType, false),
      StructField("Kafka_offset", LongType, false),
      StructField("Kafka_CreateTime", LongType, false),
      StructField("timestampType", IntegerType, false)
    )
  )

  // Assertions
  assert(resultDF.schema == expectedSchema) // Verify schema
  assert(resultDF.count() == 2)             // Verify row count
  val resultKeys = resultDF.collect().map(row => row.getAs[String]("key"))
  assert(resultKeys.contains("key1"))
  assert(resultKeys.contains("key2"))

  // Print the result for debugging
  resultDF.show()

  spark.stop()
}
