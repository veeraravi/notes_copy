from pyspark.sql import SparkSession
from pyspark.sql.functions import min

# Create a Spark session
spark = SparkSession.builder \
    .appName("Oldest Date with Least Price") \
    .getOrCreate()

# Define the price dataframe
price_data = [
    (1001, 100, "01-01-2020"),
    (1001, 200, "01-01-2020"),
    (1001, 300, "02-01-2020"),
    (1002, 200, "01-01-2020"),
    (1002, 300, "01-01-2020"),
    (1002, 400, "02-01-2020"),
    (1003, 300, "01-01-2020"),
    (1003, 400, "01-01-2020"),
    (1003, 500, "02-01-2020")
]

price_df = spark.createDataFrame(price_data, ["prod_id", "prod_price", "sale_time"])

# Define the product dataframe
product_data = [
    (1001, "apple"),
    (1002, "orange"),
    (1003, "banana")
]

product_df = spark.createDataFrame(product_data, ["prod_id", "prod_name"])

# Join price and product dataframes
joined_df = price_df.join(product_df, "prod_id", "inner")

# Group by prod_id and find the oldest date with least price
result_df = joined_df.groupBy("prod_id", "prod_name").agg(min("sale_time").alias("oldest_date"), min("prod_price").alias("least_price"))

# Filter for rows matching the oldest date and least price
final_df = result_df.join(joined_df, ["prod_id", "prod_name", "oldest_date", "least_price"], "inner")

# Select the required columns
final_df.select("prod_id", "prod_name").show(truncate=False)

# Stop the Spark session
spark.stop()
