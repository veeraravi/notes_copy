from pyspark.sql import SparkSession
from pyspark.sql.functions import min

# Create a Spark session
spark = SparkSession.builder \
    .appName("Oldest Date with Least Price") \
    .getOrCreate()

# Define the price dataframe
price_data = [
    (1001, 100, "01-01-2020"),
    (1001, 200, "01-01-2020"),
    (1001, 300, "02-01-2020"),
    (1002, 200, "01-01-2020"),
    (1002, 300, "01-01-2020"),
    (1002, 400, "02-01-2020"),
    (1003, 300, "01-01-2020"),
    (1003, 400, "01-01-2020"),
    (1003, 500, "02-01-2020")
]

price_df = spark.createDataFrame(price_data, ["prod_id", "prod_price", "sale_time"])

# Define the product dataframe
product_data = [
    (1001, "apple"),
    (1002, "orange"),
    (1003, "banana")
]

product_df = spark.createDataFrame(product_data, ["prod_id", "prod_name"])

# Join price and product dataframes
joined_df = price_df.join(product_df, "prod_id", "inner")

# Group by prod_id and find the oldest date with least price
result_df = joined_df.groupBy("prod_id", "prod_name").agg(min("sale_time").alias("oldest_date"), min("prod_price").alias("least_price"))

# Filter for rows matching the oldest date and least price
final_df = result_df.join(joined_df, ["prod_id", "prod_name", "oldest_date", "least_price"], "inner")

# Select the required columns
final_df.select("prod_id", "prod_name").write.mode("overwrite").csv("results.csv", header=True)

# Stop the Spark session
spark.stop()

import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val spark = SparkSession.builder().appName("YourAppName").getOrCreate()

// Create a sample DataFrame with 7 columns and 3 rows
val data = Seq(
  (1, 10, 5, 20, 15, 7, 25),
  (2, 15, 8, 18, 12, 9, 30),
  (3, 20, 3, 22, 14, 11, 35)
)

val columns = Seq("col1", "col2", "col3", "col4", "col5", "col6", "col7")
val df = spark.createDataFrame(data).toDF(columns: _*)

// Specify a string with 4 columns for demonstration
val str = "sum(col1)|min(col2)|max(col3)|avg(col4)"

// Define a UDF to apply various aggregate operations dynamically
val applyOperationUDF: UserDefinedFunction = udf { (col: String, operation: String) =>
  operation match {
    case "sum" => sum(col)
    case "min" => min(col)
    case "max" => max(col)
    case "avg" => avg(col)
    case "count" => count(col)
    case "first" => first(col)
    case "last" => last(col)
    case "stddev" => stddev(col)
    case "variance" => variance(col)
    // Add more cases for other aggregate functions as needed
    case _ => col
  }
}

// Apply operations and add columns to the DataFrame
val resultDF = str.split("\\|").foldLeft(df) { (tempDF, operation) =>
  val Array(op, colName) = operation.stripMargin.split("\\(")
  tempDF.withColumn(colName.dropRight(1), applyOperationUDF(col(colName.dropRight(1)), lit(op)))
}

// Select the original columns and create struct type columns only for the specified ones
val selectedColumns = str.split("\\|").map(_.split("\\(")(1).dropRight(1))
val structTypeColumns = selectedColumns.map(colName => col(colName).alias(colName))

val finalDF = resultDF.select(columns.map(col) ++ structTypeColumns: _*)

finalDF.show()

