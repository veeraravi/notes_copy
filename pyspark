from pyspark.sql import SparkSession
from pyspark.sql.functions import min

# Create a Spark session
spark = SparkSession.builder \
    .appName("Oldest Date with Least Price") \
    .getOrCreate()

# Define the price dataframe
price_data = [
    (1001, 100, "01-01-2020"),
    (1001, 200, "01-01-2020"),
    (1001, 300, "02-01-2020"),
    (1002, 200, "01-01-2020"),
    (1002, 300, "01-01-2020"),
    (1002, 400, "02-01-2020"),
    (1003, 300, "01-01-2020"),
    (1003, 400, "01-01-2020"),
    (1003, 500, "02-01-2020")
]

price_df = spark.createDataFrame(price_data, ["prod_id", "prod_price", "sale_time"])

# Define the product dataframe
product_data = [
    (1001, "apple"),
    (1002, "orange"),
    (1003, "banana")
]

product_df = spark.createDataFrame(product_data, ["prod_id", "prod_name"])

# Join price and product dataframes
joined_df = price_df.join(product_df, "prod_id", "inner")

# Group by prod_id and find the oldest date with least price
result_df = joined_df.groupBy("prod_id", "prod_name").agg(min("sale_time").alias("oldest_date"), min("prod_price").alias("least_price"))

# Filter for rows matching the oldest date and least price
final_df = result_df.join(joined_df, ["prod_id", "prod_name", "oldest_date", "least_price"], "inner")

# Select the required columns
final_df.select("prod_id", "prod_name").write.mode("overwrite").csv("results.csv", header=True)

# Stop the Spark session
spark.stop()

import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val spark = SparkSession.builder().appName("YourAppName").getOrCreate()

// Create a sample DataFrame with 7 columns and 3 rows
val data = Seq(
  (1, 10, 5, 20, 15, 7, 25),
  (2, 15, 8, 18, 12, 9, 30),
  (3, 20, 3, 22, 14, 11, 35)
)

val columns = Seq("col1", "col2", "col3", "col4", "col5", "col6", "col7")
val df = spark.createDataFrame(data).toDF(columns: _*)

// Specify a string with 4 columns for demonstration
val str = "sum(col1)|min(col2)|max(col3)|avg(col4)"

// Define a UDF to apply various aggregate operations dynamically
val applyOperationUDF: UserDefinedFunction = udf { (col: String, operation: String) =>
  operation match {
    case "sum" => sum(col)
    case "min" => min(col)
    case "max" => max(col)
    case "avg" => avg(col)
    case "count" => count(col)
    case "first" => first(col)
    case "last" => last(col)
    case "stddev" => stddev(col)
    case "variance" => variance(col)
    // Add more cases for other aggregate functions as needed
    case _ => col
  }
}

// Apply operations and add columns to the DataFrame
val resultDF = str.split("\\|").foldLeft(df) { (tempDF, operation) =>
  val Array(op, colName) = operation.stripMargin.split("\\(")
  tempDF.withColumn(colName.dropRight(1), applyOperationUDF(col(colName.dropRight(1)), lit(op)))
}

// Select the original columns and create struct type columns only for the specified ones
val selectedColumns = str.split("\\|").map(_.split("\\(")(1).dropRight(1))
val structTypeColumns = selectedColumns.map(colName => col(colName).alias(colName))

val finalDF = resultDF.select(columns.map(col) ++ structTypeColumns: _*)

finalDF.show()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val spark = SparkSession.builder().appName("YourAppName").getOrCreate()

// Create a sample DataFrame with 7 columns and 3 rows
val data = Seq(
  (1, 10, 5, 20, 15, 7, 25),
  (2, 15, 8, 18, 12, 9, 30),
  (3, 20, 3, 22, 14, 11, 35)
)

val columns = Seq("col1", "col2", "col3", "col4", "col5", "col6", "col7")
val df = spark.createDataFrame(data).toDF(columns: _*)

// Specify a string with 4 columns for demonstration
val str = "sum(col1)|min(col2)|max(col3)|avg(col4)"

// Create a map to associate each operation with its corresponding aggregate function
val operationMap: Map[String, Column => Column] = Map(
  "sum" -> sum,
  "min" -> min,
  "max" -> max,
  "avg" -> avg
  // Add more cases for other aggregate functions as needed
)

// Apply operations and add columns to the DataFrame
val resultDF = str.split("\\|").foldLeft(df) { (tempDF, operation) =>
  val Array(op, colName) = operation.stripMargin.split("\\(")
  val aggregateFunction = operationMap.getOrElse(op, identity[Column](_))
  tempDF.withColumn(colName.dropRight(1), aggregateFunction(col(colName.dropRight(1))))
}

// Create struct type columns only for the specified ones
val structTypeColumns = str.split("\\|").map { operation =>
  val Array(_, colName) = operation.stripMargin.split("\\(")
  col(colName.dropRight(1)).cast(DoubleType).alias(colName.dropRight(1))
}

val finalDF = resultDF.select(columns.map(col) ++ structTypeColumns: _*)

finalDF.show()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val spark = SparkSession.builder().appName("YourAppName").getOrCreate()

// Create a sample DataFrame with 7 columns and 3 rows
val data = Seq(
  (1, 10, 5, 20, 15, 7, 25),
  (2, 15, 8, 18, 12, 9, 30),
  (3, 20, 3, 22, 14, 11, 35)
)

val columns = Seq("col1", "col2", "col3", "col4", "col5", "col6", "col7")
val df = spark.createDataFrame(data).toDF(columns: _*)

// Specify a string with 4 columns for demonstration
val str = "sum(col1)|min(col2)|max(col3)|avg(col4)"

// Apply operations and add columns to the DataFrame
val resultDF = str.split("\\|").foldLeft(df) { (tempDF, operation) =>
  val Array(op, colName) = operation.stripMargin.split("\\(")
  tempDF.withColumn(colName.dropRight(1), functions.expr(s"$op($colName)").cast(DoubleType))
}

// Create struct type columns only for the specified ones
val structTypeColumns = str.split("\\|").map { operation =>
  val Array(_, colName) = operation.stripMargin.split("\\(")
  col(colName.dropRight(1)).cast(DoubleType).alias(colName.dropRight(1))
}

val finalDF = resultDF.select(columns.map(col) ++ structTypeColumns: _*)

finalDF.show()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val spark = SparkSession.builder().appName("YourAppName").getOrCreate()

// Create a sample DataFrame with 7 columns and 3 rows
val data = Seq(
  (1, 10, 5, 20, 15, 7, 25),
  (2, 15, 8, 18, 12, 9, 30),
  (3, 20, 3, 22, 14, 11, 35)
)

val columns = Seq("col1", "col2", "col3", "col4", "col5", "col6", "col7")
val df = spark.createDataFrame(data).toDF(columns: _*)

// Specify a string with 4 columns for demonstration
val str = "sum(col1)|min(col2)|max(col3)|avg(col4)"

// Apply operations and add columns to the DataFrame
val resultDF = str.split("\\|").foldLeft(df) { (tempDF, operation) =>
  val pattern = "(\\w+)\\((\\w+)".r
  operation.trim match {
    case pattern(op, colName) =>
      tempDF.withColumn(colName, functions.expr(s"$op($colName)").cast(DoubleType))
    case _ => tempDF
  }
}

// Create struct type columns only for the specified ones
val structTypeColumns = str.split("\\|").flatMap { operation =>
  val pattern = "(\\w+)\\((\\w+)".r
  operation.trim match {
    case pattern(_, colName) => Some(col(colName).cast(DoubleType).alias(colName))
    case _ => None
  }
}

val finalDF = resultDF.select(columns.map(col) ++ structTypeColumns: _*)

finalDF.show()


import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val spark = SparkSession.builder().appName("YourAppName").getOrCreate()

// Create a sample DataFrame with 7 columns and 3 rows
val data = Seq(
  (1, 10, 5, 20, 15, 7, 25),
  (2, 15, 8, 18, 12, 9, 30),
  (3, 20, 3, 22, 14, 11, 35)
)

val columns = Seq("col1", "col2", "col3", "col4", "col5", "col6", "col7")
val df = spark.createDataFrame(data).toDF(columns: _*)

// Specify a string with 4 columns for demonstration
val str = "sum(col1)|min(col2)|max(col3)|avg(col4)"

// Apply operations and add columns to the DataFrame
val resultDF = str.split("\\|").foldLeft(df) { (tempDF, operation) =>
  val colName = substring_index(substring_index(operation, "(", -1), ")", 1)
  val op = substring_index(operation, "(", 1)

  tempDF.withColumn(colName, expr(s"$op($colName)").cast(DoubleType))
}

// Create struct type columns only for the specified ones
val structTypeColumns = str.split("\\|").flatMap { operation =>
  val colName = substring_index(substring_index(operation, "(", -1), ")", 1)
  Some(col(colName).cast(DoubleType).alias(colName))
}

val finalDF = resultDF.select(columns.map(col) ++ structTypeColumns: _*)

finalDF.show()
--------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType
import json
import re

# Create a Spark session
spark = SparkSession.builder.appName("JsonDataProcessing").getOrCreate()

# Sample DataFrame
data = [(1, 'mydata', "<p>{'MegaAttributes': {'lastUpdateUser': 'gessner.tc', 'lastUpdateTimestamp': '2023-09-04 13: 16:10.790963', 'renderAsTable': 'false', 'tableTitle': 'MegaAttributes', 'attributes': []}, 'SourceArticleAttributes':{'lastUpdateUser': 'gessner.tc','lastUpdateTimestamp': '2023-09-04 13: 16:10.790963', 'renderAsTable': 'false', 'attributes': [{'key': 'sourceArticleID', 'value': 1295}, {'key': 'sourceArticleName', 'value': 'eBusiness Analytics - online Availability'}, {'key': 'sourceArticleParentID', 'value': ''}, {'key': 'sourceArticleTemplateID', 'value': [70]}, {'key': 'sourceArticleTemplateName', 'value': ['Data Signals Inâ€™]}]}}</p>"),]

columns = ['id', 'name', 'json_data']
df = spark.createDataFrame(data, columns)

# Define a regular expression pattern to identify HTML tags
html_pattern = re.compile(r'<.*?>')

# Function to remove HTML tags using regular expression
def remove_html_tags(json_with_html):
    return re.sub(html_pattern, '', json_with_html)

# Register the UDF
remove_html_tags_udf = udf(remove_html_tags, StringType())

# Apply the UDF to the 'json_data' column to remove HTML tags
df = df.withColumn('json_data', remove_html_tags_udf(col('json_data')))

# Define the JSON schema for the 'json_data' column
json_schema = StructType([
    StructField("MegaAttributes", StructType([
        StructField("lastUpdateUser", StringType(), True),
        StructField("lastUpdateTimestamp", StringType(), True),
        StructField("renderAsTable", StringType(), True),
        StructField("tableTitle", StringType(), True),
        StructField("attributes", ArrayType(StructType([])), True),
    ]), True),
    
    StructField("SourceArticleAttributes", StructType([
        StructField("lastUpdateUser", StringType(), True),
        StructField("lastUpdateTimestamp", StringType(), True),
        StructField("renderAsTable", StringType(), True),
        StructField("attributes", ArrayType(StructType([
            StructField("key", StringType(), True),
            StructField("value", StringType(), True)
        ])), True),
    ]), True)
])

# Convert the 'json_data' column to JSON type
df = df.withColumn('json_data', from_json(col('json_data'), json_schema))

# Display the resulting DataFrame
df.show(truncate=False)



from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType
import json
import re

# Create a Spark session
spark = SparkSession.builder.appName("JsonDataProcessing").getOrCreate()

# Corrected sample DataFrame
data = [(1, 'mydata', '<p>{"MegaAttributes": {"lastUpdateUser": "gessner.tc", "lastUpdateTimestamp": "2023-09-04 13:16:10.790963", "renderAsTable": "false", "tableTitle": "MegaAttributes", "attributes": []}, "SourceArticleAttributes": {"lastUpdateUser": "gessner.tc", "lastUpdateTimestamp": "2023-09-04 13:16:10.790963", "renderAsTable": "false", "attributes": [{"key": "sourceArticleID", "value": 1295}, {"key": "sourceArticleName", "value": "eBusiness Analytics - online Availability"}, {"key": "sourceArticleParentID", "value": ""}, {"key": "sourceArticleTemplateID", "value": [70]}, {"key": "sourceArticleTemplateName", "value": ["Data Signals In"]}]}}</p>')]

columns = ['id', 'name', 'json_data']
df = spark.createDataFrame(data, columns)

# Define a regular expression pattern to identify HTML tags
html_pattern = re.compile(r'<.*?>')

# Function to remove HTML tags using regular expression and parse JSON
def remove_html_tags_and_parse(json_with_html):
    # Remove HTML tags
    clean_json = re.sub(html_pattern, '', json_with_html)

    # Parse JSON
    try:
        json_data = json.loads(clean_json)
        return json.dumps(json_data)
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON: {e}")
        return None

# Register the UDF
remove_html_tags_and_parse_udf = udf(remove_html_tags_and_parse, StringType())

# Apply the UDF to the 'json_data' column to remove HTML tags and parse JSON
df = df.withColumn('json_data', remove_html_tags_and_parse_udf(col('json_data')))

# Define the JSON schema for the 'json_data' column
json_schema = StructType([
    StructField("MegaAttributes", StructType([
        StructField("lastUpdateUser", StringType(), True),
        StructField("lastUpdateTimestamp", StringType(), True),
        StructField("renderAsTable", StringType(), True),
        StructField("tableTitle", StringType(), True),
        StructField("attributes", ArrayType(StructType([])), True),
    ]), True),
    
    StructField("SourceArticleAttributes", StructType([
        StructField("lastUpdateUser", StringType(), True),
        StructField("lastUpdateTimestamp", StringType(), True),
        StructField("renderAsTable", StringType(), True),
        StructField("attributes", ArrayType(StructType([
            StructField("key", StringType(), True),
            StructField("value", StringType(), True)
        ])), True),
    ]), True)
])

# Convert the 'json_data' column to JSON type
df = df.withColumn('json_data', from_json(col('json_data'), json_schema))

# Display the resulting DataFrame
df.show(truncate=False)








