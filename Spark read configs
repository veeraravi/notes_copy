To read an XML file in Scala using Spark, you need to use the spark-xml library from Databricks. Here‚Äôs how you can do it step by step, including:
	‚Ä¢	Maven dependency
	‚Ä¢	XML file structure
	‚Ä¢	Scala code with .option() settings

‚∏ª

‚úÖ 1. Add spark-xml Dependency

If using sbt:

libraryDependencies += "com.databricks" %% "spark-xml" % "0.15.0"

Or in spark-submit:

--packages com.databricks:spark-xml_2.12:0.15.0


‚∏ª

‚úÖ 2. Sample XML File

Assume you have an XML file like this:

<books>
  <book>
    <id>1</id>
    <title>Scala Programming</title>
    <author>Martin Odersky</author>
  </book>
  <book>
    <id>2</id>
    <title>Spark Essentials</title>
    <author>Holden Karau</author>
  </book>
</books>

Save this as books.xml in HDFS, S3, or local file system.

‚∏ª

‚úÖ 3. Scala Spark Code to Read XML

import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("Read XML")
  .master("local[*]")
  .getOrCreate()

val df = spark.read
  .format("xml")
  .option("rowTag", "book")          // Required: denotes the repeating tag
  .option("rootTag", "books")        // Optional: root element
  .option("inferSchema", "true")     // Infer data types
  .option("charset", "UTF-8")        // Character encoding
  .load("path/to/books.xml")

df.show()


‚∏ª

‚úÖ 4. Output

+---+------------------+-------------------+
| id|             title|             author|
+---+------------------+-------------------+
|  1|  Scala Programming|  Martin Odersky  |
|  2|   Spark Essentials|    Holden Karau  |
+---+------------------+-------------------+


‚∏ª

‚úÖ 5. Other Common .option() Settings for XML

Option	Description
rowTag	The XML tag that identifies each row (e.g., book)
rootTag	Optional outer wrapper tag
attributePrefix	Prefix for attributes (default: _)
valueTag	Tag name for element text (default: _VALUE)
mode	PERMISSIVE, FAILFAST, DROPMALFORMED
ignoreSurroundingSpaces	Trims whitespace
excludeAttribute	Whether to ignore XML attributes
rowValidationXSDPath	Path to XSD schema for validation


‚∏ª

Would you like an example of reading nested or attribute-based XML as well?



Great! Let‚Äôs break this down clearly.

‚∏ª

‚úÖ Mandatory Option When Reading XML in Spark

‚úîÔ∏è rowTag is mandatory
	‚Ä¢	It tells Spark which XML tag should be treated as a single row (record).
	‚Ä¢	Without rowTag, Spark will throw an error like:
‚ùó Please set the rowTag option.

‚∏ª

‚úÖ Optional but Useful Options

Option	Required	Description
rowTag	‚úÖ Yes	XML tag that defines each record (e.g., book)
inferSchema	‚ùå No	Automatically infers schema (true/false)
rootTag	‚ùå No	Top-level wrapper tag (useful for writing back XML)
attributePrefix	‚ùå No	Prefix used for attributes (_ by default)
valueTag	‚ùå No	Tag used for element text (_VALUE by default)
excludeAttribute	‚ùå No	If true, ignores all attributes
ignoreSurroundingSpaces	‚ùå No	Trims leading/trailing spaces
mode	‚ùå No	PERMISSIVE (default), FAILFAST, DROPMALFORMED


‚∏ª

üìò Example 1: Reading Nested XML with Attributes

üî∏ Sample XML (orders.xml):

<orders>
  <order id="101" status="shipped">
    <customer>John Doe</customer>
    <items>
      <item>
        <name>Laptop</name>
        <price>1200</price>
      </item>
      <item>
        <name>Mouse</name>
        <price>25</price>
      </item>
    </items>
  </order>
</orders>


‚∏ª

üî∏ Scala Code:

val df = spark.read
  .format("xml")
  .option("rowTag", "order")
  .option("inferSchema", "true")
  .option("attributePrefix", "_")  // attribute 'id' becomes '_id'
  .option("valueTag", "_VALUE")
  .load("path/to/orders.xml")

df.printSchema()
df.show(truncate = false)


‚∏ª

üî∏ Output:

root
 |-- _id: long (nullable = true)
 |-- _status: string (nullable = true)
 |-- customer: string (nullable = true)
 |-- items: struct (nullable = true)
 |    |-- item: array (nullable = true)
 |    |    |-- element: struct
 |    |    |    |-- name: string
 |    |    |    |-- price: double


‚∏ª

üìò Example 2: Flat XML Without Attributes

üî∏ XML:

<employees>
  <employee>
    <name>Alice</name>
    <dept>HR</dept>
  </employee>
  <employee>
    <name>Bob</name>
    <dept>IT</dept>
  </employee>
</employees>

üî∏ Scala Code:

val df = spark.read
  .format("xml")
  .option("rowTag", "employee")   // required
  .option("inferSchema", "true")
  .load("path/to/employees.xml")

df.show()


‚∏ª

‚úÖ Summary

Scenario	Required Option	Notes
Basic XML	rowTag	Without this Spark fails
With attributes	attributePrefix	Defaults to _
Nested elements	Handled automatically	Results in structs/arrays
Flat XML	Only rowTag needed	Infer schema if needed


‚∏ª

Let me know if you‚Äôd like to write XML files as well (df.write.format("xml")), or validate against an XSD schema.







Below are code examples for using .option() with spark.read for each supported format in Apache Spark:

‚∏ª

‚úÖ 1. CSV Format

df = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .option("delimiter", ",") \
    .option("quote", "\"") \
    .option("escape", "\\") \
    .option("multiLine", "true") \
    .option("encoding", "UTF-8") \
    .option("nullValue", "NA") \
    .option("dateFormat", "yyyy-MM-dd") \
    .option("timestampFormat", "yyyy-MM-dd HH:mm:ss") \
    .option("mode", "PERMISSIVE") \
    .option("comment", "#") \
    .option("ignoreLeadingWhiteSpace", "true") \
    .option("ignoreTrailingWhiteSpace", "true") \
    .load("s3a://my-bucket/data.csv")


‚∏ª

‚úÖ 2. Parquet Format

df = spark.read.format("parquet") \
    .option("mergeSchema", "true") \
    .option("pathGlobFilter", "*.parquet") \
    .option("recursiveFileLookup", "true") \
    .option("datetimeRebaseMode", "CORRECTED") \
    .load("s3a://my-bucket/parquet-data/")


‚∏ª

‚úÖ 3. JSON Format

df = spark.read.format("json") \
    .option("multiLine", "true") \
    .option("primitivesAsString", "false") \
    .option("allowComments", "true") \
    .option("allowUnquotedFieldNames", "true") \
    .option("mode", "PERMISSIVE") \
    .option("columnNameOfCorruptRecord", "_corrupt") \
    .option("encoding", "UTF-8") \
    .option("dateFormat", "yyyy-MM-dd") \
    .option("timestampFormat", "yyyy-MM-dd'T'HH:mm:ss") \
    .option("dropFieldIfAllNull", "true") \
    .load("s3a://my-bucket/data.json")


‚∏ª

‚úÖ 4. Avro Format

df = spark.read.format("avro") \
    .option("avroSchema", '{"type":"record","name":"User","fields":[{"name":"name","type":"string"}]}') \
    .option("recordName", "User") \
    .option("recordNamespace", "com.example") \
    .option("compression", "snappy") \
    .option("ignoreExtension", "false") \
    .load("s3a://my-bucket/avro-data/")


‚∏ª

‚úÖ 5. JDBC Format (e.g., PostgreSQL)

df = spark.read.format("jdbc") \
    .option("url", "jdbc:postgresql://host:5432/mydb") \
    .option("dbtable", "public.customers") \
    .option("user", "myuser") \
    .option("password", "mypassword") \
    .option("driver", "org.postgresql.Driver") \
    .option("fetchsize", "1000") \
    .option("partitionColumn", "customer_id") \
    .option("lowerBound", "1") \
    .option("upperBound", "100000") \
    .option("numPartitions", "4") \
    .option("sessionInitStatement", "SET search_path TO my_schema") \
    .option("pushDownPredicate", "true") \
    .load()


‚∏ª

‚úÖ 6. MongoDB Format

df = spark.read.format("mongodb") \
    .option("uri", "mongodb://localhost:27017") \
    .option("database", "salesdb") \
    .option("collection", "orders") \
    .option("pipeline", '[{"$match": {"status": "completed"}}]') \
    .option("readPreference.name", "secondaryPreferred") \
    .option("partitioner", "MongoSamplePartitioner") \
    .option("partitionerOptions.partitionKey", "_id") \
    .option("partitionerOptions.numberOfPartitions", "4") \
    .option("spark.mongodb.read.schema", '{"order_id":"string","amount":"double"}') \
    .option("spark.mongodb.read.sampleSize", "1000") \
    .load()


‚∏ª

‚úÖ 7. ORC Format

df = spark.read.format("orc") \
    .option("mergeSchema", "true") \
    .option("recursiveFileLookup", "true") \
    .option("pathGlobFilter", "*.orc") \
    .option("datetimeRebaseMode", "LEGACY") \
    .load("s3a://my-bucket/orc-data/")


‚∏ª

‚úÖ 8. XML Format (Requires spark-xml)

Maven: "com.databricks:spark-xml_2.12:0.15.0"

df = spark.read.format("xml") \
    .option("rowTag", "book") \
    .option("rootTag", "library") \
    .option("attributePrefix", "_") \
    .option("valueTag", "_VALUE") \
    .option("charset", "UTF-8") \
    .option("ignoreSurroundingSpaces", "true") \
    .option("excludeAttribute", "false") \
    .option("mode", "PERMISSIVE") \
    .option("inferSchema", "true") \
    .load("s3a://my-bucket/books.xml")


‚∏ª

Would you like similar examples for writing to these formats using .write.option() as well?



Here‚Äôs a comprehensive list of all .option() parameters you can specify while reading data files using spark.read.option() in Apache Spark, categorized by data format:

‚∏ª

‚úÖ 1. CSV

spark.read.format("csv").option(...).load("path")

Option	Description
header	Whether the first line is header (true/false)
inferSchema	Automatically infer data types (true/false)
delimiter	Field delimiter (e.g., ,, `
quote	Quote character (e.g., ")
escape	Escape character
multiLine	Support multi-line records (true/false)
encoding	Character encoding (e.g., UTF-8)
nullValue	Treat this string as null
dateFormat	Format for date fields
timestampFormat	Format for timestamp fields
mode	PERMISSIVE, DROPMALFORMED, FAILFAST
comment	Skip lines starting with this character
ignoreLeadingWhiteSpace / ignoreTrailingWhiteSpace	Ignore white space around fields
maxCharsPerColumn	Max chars per column


‚∏ª

‚úÖ 2. Parquet

spark.read.format("parquet").option(...).load("path")

Option	Description
mergeSchema	Merge schemas across files (true/false)
pathGlobFilter	Glob pattern to filter input files
recursiveFileLookup	Recursively load files from subdirectories
datetimeRebaseMode	CORRECTED, LEGACY, EXCEPTION
int96RebaseMode	Same as above, for INT96 timestamps


‚∏ª

‚úÖ 3. JSON

spark.read.format("json").option(...).load("path")

Option	Description
multiLine	If each record spans multiple lines
primitivesAsString	Infer primitives as string
allowComments	Allow Java/C++ style comments
allowUnquotedFieldNames	Allow field names without quotes
mode	PERMISSIVE, DROPMALFORMED, FAILFAST
columnNameOfCorruptRecord	Store corrupt records in a column
encoding	Character encoding (e.g., UTF-8)
dateFormat	Date format
timestampFormat	Timestamp format
dropFieldIfAllNull	Drop fields if all null


‚∏ª

‚úÖ 4. Avro

spark.read.format("avro").option(...).load("path")

Option	Description
avroSchema	Specify custom Avro schema
recordName	Name of Avro record
recordNamespace	Namespace of Avro record
compression	Compression codec for reading/writing
ignoreExtension	Ignore .avro extension requirement
mode	Read mode (if malformed)


‚∏ª

‚úÖ 5. JDBC (RDBMS)

spark.read.format("jdbc").option(...).load()

Option	Description
url	JDBC URL (required)
dbtable or query	Table name or subquery
user / password	Credentials
driver	JDBC driver class
fetchsize	Fetch size (default 1000)
partitionColumn	Column used for partitioning
lowerBound / upperBound	Range for partition column
numPartitions	Number of partitions
sessionInitStatement	SQL to initialize session
pushDownPredicate	Enable predicate pushdown
customSchema	Manually define schema
isolationLevel	Transaction isolation (e.g., READ_COMMITTED)


‚∏ª

‚úÖ 6. MongoDB (with Spark Connector v10+)

spark.read.format("mongodb").option(...).load()

Option	Description
uri	Full MongoDB connection URI
database	MongoDB database name
collection	Collection to read
pipeline	Aggregation pipeline (JSON array)
readPreference.name	e.g., primary, secondary
partitioner	MongoSamplePartitioner, etc.
partitionerOptions.partitionKey	Field used for partitioning
partitionerOptions.numberOfPartitions	Number of partitions
spark.mongodb.read.schema	Specify schema
spark.mongodb.read.sampleSize	For schema inference


‚∏ª

‚úÖ 7. ORC

spark.read.format("orc").option(...).load("path")

Option	Description
mergeSchema	Merge schemas across ORC files
recursiveFileLookup	Read all nested files
pathGlobFilter	Filter files using glob pattern
datetimeRebaseMode	Same as Parquet (CORRECTED, etc.)


‚∏ª

‚úÖ 8. XML (via spark-xml package)

Maven: com.databricks:spark-xml_2.12:0.15.0

spark.read.format("xml").option(...).load("path")

Option	Description
rowTag	Name of the tag that represents a row (required)
rootTag	Optional wrapper tag for the whole document
attributePrefix	Prefix for attributes (default: _)
valueTag	Name of tag containing value (default: _VALUE)
charset	Encoding (e.g., UTF-8)
ignoreSurroundingSpaces	Trim spaces
excludeAttribute	Ignore attributes
mode	PERMISSIVE, FAILFAST, DROPMALFORMED
inferSchema	Automatically infer data types
rowValidationXSDPath	Validate against XSD schema


‚∏ª

üîÑ Common Options (Apply to All File Formats)

Option	Description
pathGlobFilter	Glob pattern to filter input files
recursiveFileLookup	Enable recursive directory scan
modifiedBefore / modifiedAfter	Read files modified before/after timestamps
basePath	Base directory for partition discovery


‚∏ª

Would you like the write options for each format too?
