Below are code examples for using .option() with spark.read for each supported format in Apache Spark:

â¸»

âœ… 1. CSV Format

df = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .option("delimiter", ",") \
    .option("quote", "\"") \
    .option("escape", "\\") \
    .option("multiLine", "true") \
    .option("encoding", "UTF-8") \
    .option("nullValue", "NA") \
    .option("dateFormat", "yyyy-MM-dd") \
    .option("timestampFormat", "yyyy-MM-dd HH:mm:ss") \
    .option("mode", "PERMISSIVE") \
    .option("comment", "#") \
    .option("ignoreLeadingWhiteSpace", "true") \
    .option("ignoreTrailingWhiteSpace", "true") \
    .load("s3a://my-bucket/data.csv")


â¸»

âœ… 2. Parquet Format

df = spark.read.format("parquet") \
    .option("mergeSchema", "true") \
    .option("pathGlobFilter", "*.parquet") \
    .option("recursiveFileLookup", "true") \
    .option("datetimeRebaseMode", "CORRECTED") \
    .load("s3a://my-bucket/parquet-data/")


â¸»

âœ… 3. JSON Format

df = spark.read.format("json") \
    .option("multiLine", "true") \
    .option("primitivesAsString", "false") \
    .option("allowComments", "true") \
    .option("allowUnquotedFieldNames", "true") \
    .option("mode", "PERMISSIVE") \
    .option("columnNameOfCorruptRecord", "_corrupt") \
    .option("encoding", "UTF-8") \
    .option("dateFormat", "yyyy-MM-dd") \
    .option("timestampFormat", "yyyy-MM-dd'T'HH:mm:ss") \
    .option("dropFieldIfAllNull", "true") \
    .load("s3a://my-bucket/data.json")


â¸»

âœ… 4. Avro Format

df = spark.read.format("avro") \
    .option("avroSchema", '{"type":"record","name":"User","fields":[{"name":"name","type":"string"}]}') \
    .option("recordName", "User") \
    .option("recordNamespace", "com.example") \
    .option("compression", "snappy") \
    .option("ignoreExtension", "false") \
    .load("s3a://my-bucket/avro-data/")


â¸»

âœ… 5. JDBC Format (e.g., PostgreSQL)

df = spark.read.format("jdbc") \
    .option("url", "jdbc:postgresql://host:5432/mydb") \
    .option("dbtable", "public.customers") \
    .option("user", "myuser") \
    .option("password", "mypassword") \
    .option("driver", "org.postgresql.Driver") \
    .option("fetchsize", "1000") \
    .option("partitionColumn", "customer_id") \
    .option("lowerBound", "1") \
    .option("upperBound", "100000") \
    .option("numPartitions", "4") \
    .option("sessionInitStatement", "SET search_path TO my_schema") \
    .option("pushDownPredicate", "true") \
    .load()


â¸»

âœ… 6. MongoDB Format

df = spark.read.format("mongodb") \
    .option("uri", "mongodb://localhost:27017") \
    .option("database", "salesdb") \
    .option("collection", "orders") \
    .option("pipeline", '[{"$match": {"status": "completed"}}]') \
    .option("readPreference.name", "secondaryPreferred") \
    .option("partitioner", "MongoSamplePartitioner") \
    .option("partitionerOptions.partitionKey", "_id") \
    .option("partitionerOptions.numberOfPartitions", "4") \
    .option("spark.mongodb.read.schema", '{"order_id":"string","amount":"double"}') \
    .option("spark.mongodb.read.sampleSize", "1000") \
    .load()


â¸»

âœ… 7. ORC Format

df = spark.read.format("orc") \
    .option("mergeSchema", "true") \
    .option("recursiveFileLookup", "true") \
    .option("pathGlobFilter", "*.orc") \
    .option("datetimeRebaseMode", "LEGACY") \
    .load("s3a://my-bucket/orc-data/")


â¸»

âœ… 8. XML Format (Requires spark-xml)

Maven: "com.databricks:spark-xml_2.12:0.15.0"

df = spark.read.format("xml") \
    .option("rowTag", "book") \
    .option("rootTag", "library") \
    .option("attributePrefix", "_") \
    .option("valueTag", "_VALUE") \
    .option("charset", "UTF-8") \
    .option("ignoreSurroundingSpaces", "true") \
    .option("excludeAttribute", "false") \
    .option("mode", "PERMISSIVE") \
    .option("inferSchema", "true") \
    .load("s3a://my-bucket/books.xml")


â¸»

Would you like similar examples for writing to these formats using .write.option() as well?



Hereâ€™s a comprehensive list of all .option() parameters you can specify while reading data files using spark.read.option() in Apache Spark, categorized by data format:

â¸»

âœ… 1. CSV

spark.read.format("csv").option(...).load("path")

Option	Description
header	Whether the first line is header (true/false)
inferSchema	Automatically infer data types (true/false)
delimiter	Field delimiter (e.g., ,, `
quote	Quote character (e.g., ")
escape	Escape character
multiLine	Support multi-line records (true/false)
encoding	Character encoding (e.g., UTF-8)
nullValue	Treat this string as null
dateFormat	Format for date fields
timestampFormat	Format for timestamp fields
mode	PERMISSIVE, DROPMALFORMED, FAILFAST
comment	Skip lines starting with this character
ignoreLeadingWhiteSpace / ignoreTrailingWhiteSpace	Ignore white space around fields
maxCharsPerColumn	Max chars per column


â¸»

âœ… 2. Parquet

spark.read.format("parquet").option(...).load("path")

Option	Description
mergeSchema	Merge schemas across files (true/false)
pathGlobFilter	Glob pattern to filter input files
recursiveFileLookup	Recursively load files from subdirectories
datetimeRebaseMode	CORRECTED, LEGACY, EXCEPTION
int96RebaseMode	Same as above, for INT96 timestamps


â¸»

âœ… 3. JSON

spark.read.format("json").option(...).load("path")

Option	Description
multiLine	If each record spans multiple lines
primitivesAsString	Infer primitives as string
allowComments	Allow Java/C++ style comments
allowUnquotedFieldNames	Allow field names without quotes
mode	PERMISSIVE, DROPMALFORMED, FAILFAST
columnNameOfCorruptRecord	Store corrupt records in a column
encoding	Character encoding (e.g., UTF-8)
dateFormat	Date format
timestampFormat	Timestamp format
dropFieldIfAllNull	Drop fields if all null


â¸»

âœ… 4. Avro

spark.read.format("avro").option(...).load("path")

Option	Description
avroSchema	Specify custom Avro schema
recordName	Name of Avro record
recordNamespace	Namespace of Avro record
compression	Compression codec for reading/writing
ignoreExtension	Ignore .avro extension requirement
mode	Read mode (if malformed)


â¸»

âœ… 5. JDBC (RDBMS)

spark.read.format("jdbc").option(...).load()

Option	Description
url	JDBC URL (required)
dbtable or query	Table name or subquery
user / password	Credentials
driver	JDBC driver class
fetchsize	Fetch size (default 1000)
partitionColumn	Column used for partitioning
lowerBound / upperBound	Range for partition column
numPartitions	Number of partitions
sessionInitStatement	SQL to initialize session
pushDownPredicate	Enable predicate pushdown
customSchema	Manually define schema
isolationLevel	Transaction isolation (e.g., READ_COMMITTED)


â¸»

âœ… 6. MongoDB (with Spark Connector v10+)

spark.read.format("mongodb").option(...).load()

Option	Description
uri	Full MongoDB connection URI
database	MongoDB database name
collection	Collection to read
pipeline	Aggregation pipeline (JSON array)
readPreference.name	e.g., primary, secondary
partitioner	MongoSamplePartitioner, etc.
partitionerOptions.partitionKey	Field used for partitioning
partitionerOptions.numberOfPartitions	Number of partitions
spark.mongodb.read.schema	Specify schema
spark.mongodb.read.sampleSize	For schema inference


â¸»

âœ… 7. ORC

spark.read.format("orc").option(...).load("path")

Option	Description
mergeSchema	Merge schemas across ORC files
recursiveFileLookup	Read all nested files
pathGlobFilter	Filter files using glob pattern
datetimeRebaseMode	Same as Parquet (CORRECTED, etc.)


â¸»

âœ… 8. XML (via spark-xml package)

Maven: com.databricks:spark-xml_2.12:0.15.0

spark.read.format("xml").option(...).load("path")

Option	Description
rowTag	Name of the tag that represents a row (required)
rootTag	Optional wrapper tag for the whole document
attributePrefix	Prefix for attributes (default: _)
valueTag	Name of tag containing value (default: _VALUE)
charset	Encoding (e.g., UTF-8)
ignoreSurroundingSpaces	Trim spaces
excludeAttribute	Ignore attributes
mode	PERMISSIVE, FAILFAST, DROPMALFORMED
inferSchema	Automatically infer data types
rowValidationXSDPath	Validate against XSD schema


â¸»

ðŸ”„ Common Options (Apply to All File Formats)

Option	Description
pathGlobFilter	Glob pattern to filter input files
recursiveFileLookup	Enable recursive directory scan
modifiedBefore / modifiedAfter	Read files modified before/after timestamps
basePath	Base directory for partition discovery


â¸»

Would you like the write options for each format too?
