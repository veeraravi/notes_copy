import org.apache.spark.sql.SparkSession
import com.google.cloud.bigquery._
import java.time.Instant
import java.time.temporal.ChronoUnit
import java.util.UUID

object SparkBQWriteSafe {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("SafeBQWrite")
      .getOrCreate()

    import spark.implicits._

    val df = Seq((1, "a"), (2, "b")).toDF("id", "value")

    val projectId = "your-project-id"
    val dataset = "your_dataset"

    // Use timestamp + UUID to generate a unique table name
    val tableSuffix = Instant.now().getEpochSecond.toString + "_" + UUID.randomUUID().toString.replace("-", "")
    val table = s"temp_table_$tableSuffix"
    val fullTableId = s"$projectId.$dataset.$table"

    println(s"üìù Writing to table: $fullTableId")

    // Write
    df.write
      .format("bigquery")
      .option("table", fullTableId)
      .mode("overwrite")
      .save()

    // Wait for job registration
    Thread.sleep(5000)

    // Use BQ client to find job for this exact table
    val bigQuery = BigQueryOptions.getDefaultInstance.getService
    val cutoff = Instant.now().minus(5, ChronoUnit.MINUTES)
    val jobs = bigQuery.listJobs(BigQuery.JobListOption.minCreationTime(cutoff.toEpochMilli))

    var matched = false
    val jobIterator = jobs.iterateAll().iterator()
    while (jobIterator.hasNext && !matched) {
      val job = jobIterator.next()
      val dest = job.getDestinationTable
      if (dest != null && dest.getTable.equals(table) && dest.getDataset.equals(dataset)) {
        matched = true
        if (job.getStatus.getError != null) {
          val error = job.getStatus.getError
          println(s"‚ùå Job failed: ${job.getJobId} - ${error.getMessage}")
          throw new RuntimeException("BigQuery job failed: " + error.getMessage)
        } else {
          println(s"‚úÖ Job succeeded: ${job.getJobId}")
        }
      }
    }

    if (!matched) {
      throw new RuntimeException("No matching BigQuery job found for table: " + table)
    }

    // Validate row count
    val tableId = TableId.of(dataset, table)
    val bqTable = bigQuery.getTable(tableId)
    val bqRowCount = bqTable.getNumRows
    val sparkRowCount = df.count()

    if (sparkRowCount != bqRowCount) {
      throw new RuntimeException(s"‚ùå Row count mismatch: Spark=$sparkRowCount, BQ=$bqRowCount")
    } else {
      println(s"‚úÖ Row count validated: $sparkRowCount rows")
    }

    spark.stop()
  }
}




import org.apache.spark.sql.SparkSession
import com.google.cloud.bigquery._
import java.time.Instant
import java.time.temporal.ChronoUnit

object SparkBQWriteValidator {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("SparkBQValidation")
      .getOrCreate()

    import spark.implicits._

    // 1. Create DataFrame (example)
    val df = Seq((1, "foo"), (2, "bar")).toDF("id", "value")

    // 2. Write to BigQuery
    val projectId = "your-project-id"
    val dataset = "your_dataset"
    val table = "temp_table"
    val fullTableId = s"$projectId.$dataset.$table"

    df.write
      .format("bigquery")
      .option("table", fullTableId)
      .mode("overwrite")
      .save()

    // 3. Wait for write to register
    Thread.sleep(5000)

    // 4. BigQuery client to check job status
    val bigQuery = BigQueryOptions.getDefaultInstance.getService

    val cutoff = Instant.now().minus(5, ChronoUnit.MINUTES)
    val jobIterator = bigQuery.listJobs(BigQuery.JobListOption.minCreationTime(cutoff.toEpochMilli)).iterateAll().iterator()

    var jobValidated = false
    while (jobIterator.hasNext && !jobValidated) {
      val job = jobIterator.next()

      if (job.getJobConfiguration.getType == JobConfiguration.Type.LOAD &&
          job.getStatus.getError == null &&
          Option(job.getDestinationTable).exists(_.getTable == table)) {

        println(s"‚úÖ BigQuery write job succeeded: ${job.getJobId}")
        jobValidated = true
      } else if (Option(job.getDestinationTable).exists(_.getTable == table)) {
        val error = job.getStatus.getError
        println(s"‚ùå BigQuery write job failed: ${job.getJobId}")
        println(s"Reason: ${error.getReason}, Message: ${error.getMessage}")
        throw new RuntimeException(s"BigQuery write failed: ${error.getMessage}")
      }
    }

    if (!jobValidated) {
      println("‚ö†Ô∏è No matching BigQuery job found to validate.")
    }

    // 5. Validate row count
    val tableId = TableId.of(dataset, table)
    val bqTable = bigQuery.getTable(tableId)
    val bqRowCount = bqTable.getNumRows
    val sparkRowCount = df.count()

    if (sparkRowCount != bqRowCount) {
      throw new RuntimeException(s"Row count mismatch: Spark=$sparkRowCount, BQ=$bqRowCount")
    } else {
      println(s"‚úÖ Row count validated: $sparkRowCount rows")
    }

    spark.stop()
  }
}
